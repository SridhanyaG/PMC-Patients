{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e9732a5c-f324-42bf-bd7f-0cb92c715c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 19 17:31:06 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   76C    P0              31W /  70W |  14746MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   3318075      C   /opt/conda/envs/llm/bin/python              638MiB |\n",
      "|    0   N/A  N/A   3326377      C   /opt/conda/envs/llm/bin/python            14104MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ded98c-7601-44fc-a344-a78701606124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63a8fa-e135-402d-a605-7693d83637e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73611e10-4abf-4feb-b1eb-8466a309e496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33556640-b8da-4c34-a92b-65bedcb6153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HUGGINGFACEHUB_API_TOKEN= \n",
    "!export HF_TOKEN= "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f68c8fa-7455-4eb5-a908-97d4420813e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ab66b-6021-47d6-b8e8-34d50d122681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ffc52-591b-486f-88ad-ccd65acf2423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130562f8-a777-4b85-a943-16b0ec3f5005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0c8015-8a03-4826-a8c1-02c561ead3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "access_token_read = \" \"\n",
    "access_token_write = \" \"\n",
    "login(token = access_token_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "763ad22d-8f69-41aa-8471-8e119dead6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n",
    "import csv\n",
    "import glob\n",
    "from transformers import AutoTokenizer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3cae28cb-4469-40d1-88f6-237ecbdc44b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "91b9fc2b-1899-4dfb-adec-fed31c4de9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clear_cuda_cache_and_gc():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Call the function\n",
    "clear_cuda_cache_and_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "95f9b22f-98ae-4786-bd00-1c9982818537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177db691-31c5-4691-8f53-b3702bb607f7",
   "metadata": {},
   "source": [
    "# PMC-Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8c33a-f67f-442b-84d7-2692880ea377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b21b86d-dbed-440c-b39e-4555cdf4fd8a",
   "metadata": {},
   "source": [
    "Patient information are presented as a dataframe \n",
    "- `patient_id`: string. A continuous id of patients, starting from 0.\n",
    "- `patient_uid`: string. Unique ID for each patient, with format PMID-x, where PMID is the PubMed Identifier of source article of the note and x denotes index of the note in source article.\n",
    "- `PMID`: string. PMID for source article.\n",
    "- `file_path`: string. File path of xml file of source article.\n",
    "- `title`: string. Source article title.\n",
    "- `patient`: string. Patient note.\n",
    "- `age`: list of tuples. Each entry is in format `(value, unit)` where value is a float number and unit is in 'year', 'month', 'week', 'day' and 'hour' indicating age unit. For example, `[[1.0, 'year'], [2.0, 'month']]` indicating the patient is a one-year- and two-month-old infant.\n",
    "- `gender`: 'M' or 'F'. Male or Female.\n",
    "- `relevant_articles`: dict. The key is PMID of the relevant articles and the corresponding value is its relevance score (2 or 1 as defined in the ``Methods'' section).\n",
    "- `similar_patients`: dict. The key is patient_uid of the similar patients and the corresponding value is its similarity score (2 or 1 as defined in the ``Methods'' section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f64934-9da7-4bb4-a89b-cd1e123d98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import fhirclient.models.patient as fhir_patient\n",
    "import fhirclient.models.condition as fhir_condition\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "import re\n",
    "import nltk\n",
    "from autocorrect import Speller\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import Speller\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa72a0-0b7f-4c78-b5f4-abc41f058c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8066c4f-047e-491a-a825-42c65c8e47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d23ab-8073-47b6-850f-f4c100ac73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "biomodel = HuggingFaceEndpoint(\n",
    "   # repo_id=\"dmis-lab/biobert-v1.1\",\n",
    "    repo_id=\"alvaroalon2/biobert_diseases_ner\",\n",
    "    temperature=0,\n",
    "    model_kwargs={\"max_length\": 180, \"device\": \"cuda\"},\n",
    "    use_auth_token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72d5ea-eec6-412f-9d49-e0193981a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = Ollama(\n",
    "    base_url='http://10.113.8.4:8086',\n",
    "    model=\"cniongolo/biomistral:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092d91c-d7f4-40f5-921c-ec2b9a19c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"zhengyun21/PMC-Patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85352519-c071-4e27-980f-2ef8be87fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f273a-7291-4b5e-96c9-e82b2e13464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aceeb8e-71f8-4500-bb35-a75e98297f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94bee3c-446f-4922-b683-e9dff7e2198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720a069-8c8e-41f9-bf1c-8c165c6c47e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e20c7-1310-43a1-8ef0-35150a0d89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b11bc3-a512-4f15-ba2e-c71f970c7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d19767-c6f7-43c4-88eb-03538848753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7bc92c-4c51-4bd7-a50c-521c0489cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age\n",
    "extract_age = lambda x: re.findall(r\"\\d+\\.\\d+\", x)[0]\n",
    "df.age = df.age.apply(extract_age).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd088c8a-d72c-447c-bab8-1354ce40584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ff355-66a8-4947-a248-8f49f368bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns=[\"file_path\", \"patient_id\", \"patient_uid\", \"relevant_articles\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0464cd2-322f-42d6-a469-0f3f00271a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_note = \"\"\"\n",
    "The patient is a 45-year-old male with a history of diabetes and hypertension.\n",
    "He was prescribed metformin and lisinopril.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e840b205-b21b-41ef-aa6e-d399a0ee0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\"\n",
    "stop_words  = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e2b51-aab9-49ff-b267-e89a468dd028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Converting text to lower case as in, converting \"Hello\" to  \"hello\" or \"HELLO\" to \"hello\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Specll check the words\n",
    "    spell  = Speller(lang='en')\n",
    "    \n",
    "    texts = spell(text)\n",
    "    \n",
    "    return ' '.join([w.lower() for w in word_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a7298a-1b74-4858-ba98-2b00b2d14697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(lower_case):\n",
    "    # split text phrases into words\n",
    "    words  = nltk.word_tokenize(lower_case)\n",
    "    \n",
    "    \n",
    "    # Create a list of all the punctuations\n",
    "    punctuations = [ '/', '!', '?', ';', ':', '(',')', '[',']', '-', '_', '%']\n",
    "    \n",
    "    # Remove all the special characters\n",
    "    punctuations = re.sub(r'\\W', ' ', str(lower_case))\n",
    "    \n",
    "    # Initialize the stopwords variable, which is a list of words ('and', 'the', 'i', 'yourself', 'is') that do not hold much values as key words\n",
    "    stop_words  = stopwords.words('english')\n",
    "    \n",
    "    # Getting rid of all the words that contain numbers in them\n",
    "    w_num = re.sub('\\w*\\d\\w*', '', lower_case).strip()\n",
    "    \n",
    "    # remove all single characters\n",
    "    lower_case = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', lower_case)\n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    lower_case = re.sub(r'\\s+', ' ', lower_case, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    lower_case = re.sub(r'^b\\s+', '', lower_case)\n",
    "    \n",
    "    # Removing non-english characters\n",
    "    lower_case = re.sub(r'^b\\s+', '', lower_case)\n",
    "    \n",
    "    # Return keywords which are not in stop words \n",
    "    keywords = [word for word in words if not word in stop_words  and word in punctuations and  word in w_num]\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d1a9e-2254-4fdd-ae66-88a1fa4905c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14f3b7-2f00-452d-8344-8de6b3f84940",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "print('We will use the GPU:', torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7880ab01-c12f-4911-9ad6-b8372d55e5f3",
   "metadata": {},
   "source": [
    "### Pending Perform NER with custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff55f1c-a5cb-4100-8f31-ff5d0bc8d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to convert a row to FHIR Patient format\n",
    "def row_to_fhir(row):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame row to FHIR Patient format.\n",
    "    \"\"\"\n",
    "    return FHIRPatient(\n",
    "        id=str(row['patient_id']),\n",
    "        name=row['patient_name'],\n",
    "        gender=row['gender'] if 'gender' in row else None,\n",
    "        birthDate=row['birthdate'] if 'birthdate' in row else None,\n",
    "        conditions=row['conditions'],\n",
    "        medications=row['medications']\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
