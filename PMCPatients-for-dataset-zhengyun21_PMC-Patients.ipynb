{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e9732a5c-f324-42bf-bd7f-0cb92c715c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 19 17:31:06 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   76C    P0              31W /  70W |  14746MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   3318075      C   /opt/conda/envs/llm/bin/python              638MiB |\n",
      "|    0   N/A  N/A   3326377      C   /opt/conda/envs/llm/bin/python            14104MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ded98c-7601-44fc-a344-a78701606124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63a8fa-e135-402d-a605-7693d83637e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73611e10-4abf-4feb-b1eb-8466a309e496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33556640-b8da-4c34-a92b-65bedcb6153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HUGGINGFACEHUB_API_TOKEN=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\n",
    "!export HF_TOKEN=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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f68c8fa-7455-4eb5-a908-97d4420813e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ab66b-6021-47d6-b8e8-34d50d122681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ffc52-591b-486f-88ad-ccd65acf2423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130562f8-a777-4b85-a943-16b0ec3f5005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0c8015-8a03-4826-a8c1-02c561ead3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "access_token_read = \"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\"\n",
    "access_token_write = \"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\"\n",
    "login(token = access_token_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2fc2e-5d54-4520-8d5b-6ff7dfe90495",
   "metadata": {},
   "source": [
    "# Training data \n",
    "wget http://nlp.dmis.korea.edu/projects/biobert-2020-checkpoints/datasets.tar.gz -O\n",
    "\n",
    "## BC2GM (BioCreative II Gene Mention Recognition):\n",
    "This dataset is used for gene mention recognition in biomedical texts. It contains sentences annotated with gene mentions, helping to identify gene names in scientific literature1.\n",
    "## BC4CHEMD (BioCreative IV Chemical Compound and Drug Name Recognition):\n",
    "This dataset includes 10,000 PubMed abstracts with 84,355 manually annotated chemical entity mentions. It’s used for recognizing chemical compounds and drug names in biomedical literature2.\n",
    "## BC5CDR (BioCreative V Chemical Disease Relation):\n",
    "## BC5CDR-chem: Focuses on chemical entities, with 4409 annotated chemicals in 1500 PubMed articles3.\n",
    "BC5CDR-disease: Focuses on disease entities, with 5818 annotated diseases in the same set of articles3.\n",
    "The dataset also includes 3116 chemical-disease interactions, making it useful for studying relationships between chemicals and diseases3.\n",
    "## JNLPBA (Joint Workshop on Natural Language Processing in Biomedicine and its Applications):\n",
    "This dataset is derived from the GENIA corpus and contains 2000 abstracts annotated with biomedical entities like proteins, DNA, RNA, cell lines, and cell types. It’s widely used for named entity recognition in biomedical texts4.\n",
    "Linnaeus:\n",
    "This dataset is designed for species name recognition in biomedical literature. It includes a variety of document formats and aims to identify and classify species names with high precision and recall5.\n",
    "## NCBI-disease:\n",
    "The NCBI Disease corpus consists of 793 PubMed abstracts annotated with disease mentions. It includes concept identifiers from MeSH or OMIM, making it a valuable resource for disease name recognition and normalization6.\n",
    "## S800:\n",
    "The S800 corpus contains 800 PubMed abstracts annotated with organism mentions, mapped to NCBI Taxonomy identifiers. It covers a diverse range of species, including bacteria, fungi, plants, and animals7.\n",
    "considering only /home/sridhanya_ganapathi_team_neustar/PMC-Patients/ddata/datasets/NER/BC5CDR-disease\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "763ad22d-8f69-41aa-8471-8e119dead6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n",
    "import csv\n",
    "import glob\n",
    "from transformers import AutoTokenizer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3cae28cb-4469-40d1-88f6-237ecbdc44b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "91b9fc2b-1899-4dfb-adec-fed31c4de9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clear_cuda_cache_and_gc():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Call the function\n",
    "clear_cuda_cache_and_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc1e841-a135-4e41-99a6-b525a3c1db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "basefolderpath='ddata/datasets/NER/'\n",
    "folders = ['BC2GM','BC4CHEMD','BC5CDR-chem','BC5CDR-disease','JNLPBA','linnaeus','NCBI-disease','s800']\n",
    "trainfilename='train_dev.tsv'\n",
    "trainfilename2='train.tsv'\n",
    "valfilename='devel.tsv'\n",
    "testfilename='test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1450fc6-312f-4dc9-8f0c-c59d92be16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_paths = []\n",
    "train_file_paths2=[]\n",
    "val_file_paths=[]\n",
    "test_file_paths=[]\n",
    "\n",
    "for folder in folders:\n",
    "    train_file_paths.append(basefolderpath+folder+\"/\"+trainfilename)\n",
    "    train_file_paths2.append(basefolderpath+folder+\"/\"+trainfilename2)\n",
    "    val_file_paths.append(basefolderpath+folder+\"/\"+valfilename)\n",
    "    test_file_paths.append(basefolderpath+folder+\"/\"+testfilename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b630a6d5-190e-4d60-80bf-2eb02b82a04f",
   "metadata": {},
   "source": [
    "## BC2GM (BioCreative II Gene Mention Recognition):\n",
    "B-GENE: Beginning of a gene mention.\n",
    "I-GENE: Inside a gene mention.\n",
    "O: Outside any gene mention.\n",
    "## BC4CHEMD (BioCreative IV Chemical Compound and Drug Name Recognition):\n",
    "B-CHEMICAL: Beginning of a chemical compound or drug name.\n",
    "I-CHEMICAL: Inside a chemical compound or drug name.\n",
    "O: Outside any chemical mention.\n",
    "## BC5CDR (BioCreative V Chemical Disease Relation):\n",
    "BC5CDR-chem:\n",
    "B-CHEMICAL: Beginning of a chemical entity.\n",
    "I-CHEMICAL: Inside a chemical entity.\n",
    "## BC5CDR-disease:\n",
    "B-DISEASE: Beginning of a disease entity.\n",
    "I-DISEASE: Inside a disease entity.\n",
    "O: Outside any chemical or disease mention.\n",
    "## JNLPBA (Joint Workshop on Natural Language Processing in Biomedicine and its Applications):\n",
    "B-PROTEIN, I-PROTEIN: For protein mentions.\n",
    "B-DNA, I-DNA: For DNA mentions.\n",
    "B-RNA, I-RNA: For RNA mentions.\n",
    "B-CELL_LINE, I-CELL_LINE: For cell line mentions.\n",
    "B-CELL_TYPE, I-CELL_TYPE: For cell type mentions.\n",
    "O: Outside any entity mention.\n",
    "## Linnaeus:\n",
    "B-SPECIES: Beginning of a species name.\n",
    "I-SPECIES: Inside a species name.\n",
    "O: Outside any species mention.\n",
    "## NCBI-disease:\n",
    "B-DISEASE: Beginning of a disease mention.\n",
    "I-DISEASE: Inside a disease mention.\n",
    "O: Outside any disease mention.\n",
    "## S800:\n",
    "B-ORGANISM: Beginning of an organism mention.\n",
    "I-ORGANISM: Inside an organism mention.\n",
    "O: Outside any organism mention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7615614-5b70-4d1f-b0c7-42fac6a290de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Value   Entity\n",
      "0  Immunohistochemical  OUTSIDE\n",
      "1             staining  OUTSIDE\n",
      "2                  was  OUTSIDE\n",
      "3             positive  OUTSIDE\n",
      "4                  for  OUTSIDE\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold DataFrames\n",
    "\n",
    "dataframes = []\n",
    "column_names = ['Value', 'Entity']\n",
    "# Loop through the file paths and read each file\n",
    "for file_path in train_file_paths:\n",
    "    df = pd.read_csv(file_path, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=column_names)  # Adjust header=None if no header\n",
    "    if 'BC2GM' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-GENE', 'I': 'I-GENE', 'O': 'OUTSIDE'})\n",
    "    elif 'BC4CHEMD' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-CHEMICAL', 'I': 'I-CHEMICAL', 'O': 'OUTSIDE'})\n",
    "    elif 'BC5CDR-chem' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-CHEMICAL', 'I': 'I-CHEMICAL', 'O': 'OUTSIDE'})\n",
    "    elif 'BC5CDR-disease' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-DISEASE', 'I': 'I-DISEASE', 'O': 'OUTSIDE'})\n",
    "    elif 'JNLPBA' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-PROTEINPROTEIN', 'I': 'I-PROTEIN', 'O': 'OUTSIDE'})\n",
    "    elif 'linnaeus' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-SPECIES', 'I': 'I-SPECIES', 'O': 'OUTSIDE'})\n",
    "    elif 'NCBI-disease' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-DISEASE', 'I': 'I-DISEASE', 'O': 'OUTSIDE'})\n",
    "    elif 's800' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-ORGANISM', 'I': 'I-ORGANISM', 'O': 'OUTSIDE'})\n",
    "                           \n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d31d3ff9-6bee-4df2-9f12-f77d82d4c8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Value   Entity\n",
      "0  Immunohistochemical  OUTSIDE\n",
      "1             staining  OUTSIDE\n",
      "2                  was  OUTSIDE\n",
      "3             positive  OUTSIDE\n",
      "4                  for  OUTSIDE\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold DataFrames\n",
    "\n",
    "tdataframes = []\n",
    "column_names = ['Entity', 'Value']\n",
    "# Loop through the file paths and read each file\n",
    "for file_path in val_file_paths:\n",
    "    df = pd.read_csv(file_path, sep='\\t', quoting=csv.QUOTE_NONE, header=None, names=column_names)  # Adjust header=None if no header\n",
    "    if 'BC2GM' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-GENE', 'I': 'I-GENE', 'O': 'OUTSIDE'})\n",
    "    elif 'BC4CHEMD' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-CHEMICAL', 'I': 'I-CHEMICAL', 'O': 'OUTSIDE'})\n",
    "    elif 'BC5CDR-chem' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-CHEMICAL', 'I': 'I-CHEMICAL', 'O': 'OUTSIDE'})\n",
    "    elif 'BC5CDR-disease' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-DISEASE', 'I': 'I-DISEASE', 'O': 'OUTSIDE'})\n",
    "    elif 'JNLPBA' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-PROTEINPROTEIN', 'I': 'I-PROTEIN', 'O': 'OUTSIDE'})\n",
    "    elif 'linnaeus' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-SPECIES', 'I': 'I-SPECIES', 'O': 'OUTSIDE'})\n",
    "    elif 'NCBI-disease' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-DISEASE', 'I': 'I-DISEASE', 'O': 'OUTSIDE'})\n",
    "    elif 's800' in file_path:\n",
    "        df['Entity'] = df['Entity'].replace({'B': 'B-ORGANISM', 'I': 'I-ORGANISM', 'O': 'OUTSIDE'})\n",
    "                           \n",
    "    tdataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "test_combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(test_combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5492325-7068-44a7-996f-2540a6a13a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3944377, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b202ca6-75ce-475c-98ed-d797e028c85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3944377, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7473366-8725-4e28-9681-e6ca0cd0cf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value     389\n",
       "Entity      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "254f2833-c343-48ef-8538-9704b18e71d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value     389\n",
       "Entity      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ecae205-3d99-4495-9d12-84298855d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = combined_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9435897c-3ad2-4a67-9ac0-c3b521aa1570",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleaned_df = test_combined_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78fe83ce-c512-4849-b612-10ef962954cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3943988, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12fb8629-87ac-430d-ab2c-16176a361f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3943988, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cleaned_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36c927f0-bd8a-4fc5-aa80-f23eed2f6cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value     object\n",
       "Entity    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9117ccc6-a337-4ce0-98af-d5d93f3a9dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entity\n",
       "B-CHEMICAL            69514\n",
       "B-DISEASE             14349\n",
       "B-GENE                18258\n",
       "B-ORGANISM             2941\n",
       "B-PROTEINPROTEIN      40753\n",
       "B-SPECIES              2830\n",
       "I-CHEMICAL            74434\n",
       "I-DISEASE             12846\n",
       "I-GENE                26537\n",
       "I-ORGANISM             3795\n",
       "I-PROTEIN             81563\n",
       "I-SPECIES              1493\n",
       "OUTSIDE             3594675\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.groupby(['Entity']).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79566dee-93c4-4695-a4bb-42cf6c48cf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entity\n",
       "B-CHEMICAL            69514\n",
       "B-DISEASE             14349\n",
       "B-GENE                18258\n",
       "B-ORGANISM             2941\n",
       "B-PROTEINPROTEIN      40753\n",
       "B-SPECIES              2830\n",
       "I-CHEMICAL            74434\n",
       "I-DISEASE             12846\n",
       "I-GENE                26537\n",
       "I-ORGANISM             3795\n",
       "I-PROTEIN             81563\n",
       "I-SPECIES              1493\n",
       "OUTSIDE             3594675\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cleaned_df.groupby(['Entity']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88da86f5-685c-47c7-9c06-a901a01f5dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies=cleaned_df['Entity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09451410-d689-46ae-ad82-43becc109763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OUTSIDE': 0,\n",
       " 'B-GENE': 1,\n",
       " 'I-GENE': 2,\n",
       " 'B-CHEMICAL': 3,\n",
       " 'I-CHEMICAL': 4,\n",
       " 'B-DISEASE': 5,\n",
       " 'I-DISEASE': 6,\n",
       " 'B-PROTEINPROTEIN': 7,\n",
       " 'I-PROTEIN': 8,\n",
       " 'B-SPECIES': 9,\n",
       " 'I-SPECIES': 10,\n",
       " 'B-ORGANISM': 11,\n",
       " 'I-ORGANISM': 12}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids = {k: v for v, k in enumerate(frequencies)}\n",
    "ids_to_labels = {v: k for v, k in enumerate(frequencies)}\n",
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222458ed-2720-4dd4-bdc0-0fd2c1148425",
   "metadata": {},
   "source": [
    "## Preparing the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "6f712f8b-6e90-43e6-9a78-5543b8342059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LEN = 128\n",
    "# TRAIN_BATCH_SIZE = 512\n",
    "# VALID_BATCH_SIZE = 128\n",
    "# EPOCHS = 1\n",
    "# LEARNING_RATE = 1e-05\n",
    "# MAX_GRAD_NORM = 10\n",
    "# # MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "36f38e65-52b6-4709-8883-2e462f1a87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "10b757c6-e6ca-4cc8-a4e3-7c6b8d1818e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels \n",
    "        sentence = self.data.Value[index].strip().split()  \n",
    "        word_labels = [self.data.Entity[index]]\n",
    "\n",
    "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
    "        encoding = self.tokenizer(sentence,\n",
    "                                  is_split_into_words=True, \n",
    "                                  return_offsets_mapping=True, \n",
    "                                  padding='max_length', \n",
    "                                  truncation=True, \n",
    "                                  max_length=self.max_len)\n",
    "        \n",
    "        # step 3: create token labels only for first word pieces of each tokenized word\n",
    "        labels = [labels_to_ids[label] for label in word_labels] \n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "        \n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "            if mapping[0] == 0 and mapping[1] != 0:\n",
    "                encoded_labels[idx] = labels[i]\n",
    "                i += 1\n",
    "\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len \n",
    "\n",
    "# Function to create DataLoader\n",
    "def create_dataloader(file_path, tokenizer, max_len, batch_size):\n",
    "    dataframe = pd.read_csv(file_path)\n",
    "    dataset = CustomDataset(dataframe, tokenizer, max_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1de7d6fe-4baa-4268-8478-a414297c8f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (3943988, 2)\n",
      "TRAIN Dataset: (3943988, 2)\n",
      "TEST Dataset: (1971994, 2)\n"
     ]
    }
   ],
   "source": [
    "drop_size = 0.5\n",
    "drop_dataset = test_cleaned_df.sample(frac=drop_size,random_state=200)\n",
    "test_dataset = test_cleaned_df.drop(drop_dataset.index).reset_index(drop=True)\n",
    "train_dataset = cleaned_df.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(cleaned_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "# training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
    "# testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "2b622711-505c-4599-a083-4065ba94bf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved in the current directory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the datasets\n",
    "train_dataset.to_csv('processeddata/train_dataset.csv', index=False)\n",
    "test_dataset.to_csv('processeddata/test_dataset.csv', index=False)\n",
    "\n",
    "print(\"Datasets saved in the current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "3afd8c68-2e08-4f58-a7c0-759200ae5921",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "training_loader = create_dataloader('processeddata/train_dataset.csv', tokenizer, max_len=128, batch_size=TRAIN_BATCH_SIZE)\n",
    "testing_loader = create_dataloader('processeddata/test_dataset.csv', tokenizer, max_len=128, batch_size=VALID_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "557b06ed-5676-4a14-8444-c1f2d099e8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OUTSIDE': 0,\n",
       " 'B-GENE': 1,\n",
       " 'I-GENE': 2,\n",
       " 'B-CHEMICAL': 3,\n",
       " 'I-CHEMICAL': 4,\n",
       " 'B-DISEASE': 5,\n",
       " 'I-DISEASE': 6,\n",
       " 'B-PROTEINPROTEIN': 7,\n",
       " 'I-PROTEIN': 8,\n",
       " 'B-SPECIES': 9,\n",
       " 'I-SPECIES': 10,\n",
       " 'B-ORGANISM': 11,\n",
       " 'I-ORGANISM': 12}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d2f86c4b-7a42-429e-b6e3-885d8c79c5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]       -100\n",
      "im          0\n",
      "##mu        -100\n",
      "##no        -100\n",
      "##his       -100\n",
      "##to        -100\n",
      "##chemical  -100\n",
      "[SEP]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n",
      "[PAD]       -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"input_ids\"]), training_set[0][\"labels\"]):\n",
    "  print('{0:10}  {1}'.format(token, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "de9606ed-6e4f-4661-9cc5-eeaf251846d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "#                 'shuffle': True,\n",
    "#                 'num_workers': 0\n",
    "#                 }\n",
    "\n",
    "# test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "#                 'shuffle': True,\n",
    "#                 'num_workers': 0\n",
    "#                 }\n",
    "\n",
    "# training_loader = DataLoader(training_set, **train_params)\n",
    "# testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6cc9ceb6-edf9-4be9-a472-d0bfa1593d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.1', \n",
    "                                                   num_labels=len(labels_to_ids), id2label=ids_to_labels,label2id=labels_to_ids)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b6914e06-3c3f-4eb8-951f-e39524687ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2635, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = training_set[2]\n",
    "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
    "labels = inputs[\"labels\"].unsqueeze(0)\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "initial_loss = outputs[0]\n",
    "initial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "17bb5b50-8afc-4f3b-add0-8a7b35a208e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "9f302587-84c4-49c8-8281-9f903a1a0b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 13])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_logits = outputs[1]\n",
    "tr_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e8d86e85-c669-49ac-a761-8c8179658d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "5024bf00-77f8-4f92-99e6-b5cf55e200c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
    "def train(epoch):\n",
    "    print(\"training_loader*****\")\n",
    "    print(training_loader)\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    print(\" model.train()*****\")\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        labels = batch['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "        tr_loss += loss.item()\n",
    "        \n",
    "        # loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        # tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "        \n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_labels.extend(labels)\n",
    "        tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8145dde6-a542-468b-b9ba-7732ef5c2afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c01d85e8-77d3-4bbf-a4ef-1f71dad8a3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "95f9b22f-98ae-4786-bd00-1c9982818537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9728369-1606-4dec-b764-ed263b41b371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "training_loader*****\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7ffa1db9a6d0>\n",
      " model.train()*****\n",
      "Training loss per 100 training steps: 3.3056726455688477\n",
      "Training loss per 100 training steps: 0.8076972484146014\n",
      "Training loss per 100 training steps: 0.609671280825909\n",
      "Training loss per 100 training steps: 0.544011575810339\n",
      "Training loss per 100 training steps: 0.503787974039664\n",
      "Training loss per 100 training steps: 0.4784373146167653\n",
      "Training loss per 100 training steps: 0.460940418575895\n",
      "Training loss per 100 training steps: 0.450889910641691\n",
      "Training loss per 100 training steps: 0.4360365275986558\n",
      "Training loss per 100 training steps: 0.4276676043686142\n",
      "Training loss per 100 training steps: 0.41529766067520124\n",
      "Training loss per 100 training steps: 0.4093970442825182\n",
      "Training loss per 100 training steps: 0.401973235213836\n",
      "Training loss per 100 training steps: 0.3955968199534017\n",
      "Training loss per 100 training steps: 0.3904570037290337\n",
      "Training loss per 100 training steps: 0.38630811059270437\n",
      "Training loss per 100 training steps: 0.3819050079026608\n",
      "Training loss per 100 training steps: 0.37911303745454583\n",
      "Training loss per 100 training steps: 0.37486188937555676\n",
      "Training loss per 100 training steps: 0.3713244349881419\n",
      "Training loss per 100 training steps: 0.3693271712090509\n",
      "Training loss per 100 training steps: 0.3670194628730466\n",
      "Training loss per 100 training steps: 0.3646789738677378\n",
      "Training loss per 100 training steps: 0.36249695633453477\n",
      "Training loss per 100 training steps: 0.3602351794754004\n",
      "Training loss per 100 training steps: 0.3582421539218032\n",
      "Training loss per 100 training steps: 0.3578352964168464\n",
      "Training loss per 100 training steps: 0.356044518645137\n",
      "Training loss per 100 training steps: 0.35546371052078224\n",
      "Training loss per 100 training steps: 0.35347549242385806\n",
      "Training loss per 100 training steps: 0.35275114665626583\n",
      "Training loss per 100 training steps: 0.3503965987153868\n",
      "Training loss per 100 training steps: 0.34912989231950137\n",
      "Training loss per 100 training steps: 0.34800054542539754\n",
      "Training loss per 100 training steps: 0.3465073066047494\n",
      "Training loss per 100 training steps: 0.3453390721957391\n",
      "Training loss per 100 training steps: 0.3441668822363089\n",
      "Training loss per 100 training steps: 0.3430902789764849\n",
      "Training loss per 100 training steps: 0.3412589702012311\n",
      "Training loss per 100 training steps: 0.339810719773731\n",
      "Training loss per 100 training steps: 0.33835185785149147\n",
      "Training loss per 100 training steps: 0.3370323144984962\n",
      "Training loss per 100 training steps: 0.33636390224455637\n",
      "Training loss per 100 training steps: 0.3357996059825457\n",
      "Training loss per 100 training steps: 0.33528564697721075\n",
      "Training loss per 100 training steps: 0.33425627666169855\n",
      "Training loss per 100 training steps: 0.33372146592323054\n",
      "Training loss per 100 training steps: 0.3327967655222718\n",
      "Training loss per 100 training steps: 0.332067837447496\n",
      "Training loss per 100 training steps: 0.3312961929252569\n",
      "Training loss per 100 training steps: 0.33045303866612097\n",
      "Training loss per 100 training steps: 0.330122688916833\n",
      "Training loss per 100 training steps: 0.32966435197103316\n",
      "Training loss per 100 training steps: 0.328658511733028\n",
      "Training loss per 100 training steps: 0.3276686184085841\n",
      "Training loss per 100 training steps: 0.3272105200037393\n",
      "Training loss per 100 training steps: 0.32668265988316986\n",
      "Training loss per 100 training steps: 0.3260225453914725\n",
      "Training loss per 100 training steps: 0.32577842995161577\n",
      "Training loss per 100 training steps: 0.3258189238199749\n",
      "Training loss per 100 training steps: 0.32546851996529813\n",
      "Training loss per 100 training steps: 0.3252798699462593\n",
      "Training loss per 100 training steps: 0.32448822170975994\n",
      "Training loss per 100 training steps: 0.32405029906814575\n",
      "Training loss per 100 training steps: 0.3232854053576271\n",
      "Training loss per 100 training steps: 0.3225169828722597\n",
      "Training loss per 100 training steps: 0.32168681895547435\n",
      "Training loss per 100 training steps: 0.32099961318003944\n",
      "Training loss per 100 training steps: 0.32059729641237417\n",
      "Training loss per 100 training steps: 0.31976533856150047\n",
      "Training loss per 100 training steps: 0.3194255669755706\n",
      "Training loss per 100 training steps: 0.31890221878163827\n",
      "Training loss per 100 training steps: 0.3184806176838542\n",
      "Training loss per 100 training steps: 0.3181388586321523\n",
      "Training loss per 100 training steps: 0.31754110721007256\n",
      "Training loss per 100 training steps: 0.31760365375783356\n",
      "Training loss per 100 training steps: 0.31725112617409396\n",
      "Training loss per 100 training steps: 0.31700905890021036\n",
      "Training loss per 100 training steps: 0.31652850313144376\n",
      "Training loss per 100 training steps: 0.3159073524580439\n",
      "Training loss per 100 training steps: 0.3157230703739377\n",
      "Training loss per 100 training steps: 0.31516764452077384\n",
      "Training loss per 100 training steps: 0.3146871879996423\n",
      "Training loss per 100 training steps: 0.314566944719753\n",
      "Training loss per 100 training steps: 0.31423887132801476\n",
      "Training loss per 100 training steps: 0.3136916311167215\n",
      "Training loss per 100 training steps: 0.31362532161285783\n",
      "Training loss per 100 training steps: 0.3133096280265306\n",
      "Training loss per 100 training steps: 0.3131700741303218\n",
      "Training loss per 100 training steps: 0.3126762029290603\n",
      "Training loss per 100 training steps: 0.3123166259421031\n",
      "Training loss per 100 training steps: 0.31185160484062624\n",
      "Training loss per 100 training steps: 0.3115293635519009\n",
      "Training loss per 100 training steps: 0.3110958664537753\n",
      "Training loss per 100 training steps: 0.3109228520787584\n",
      "Training loss per 100 training steps: 0.31047323732909865\n",
      "Training loss per 100 training steps: 0.3103673344944788\n",
      "Training loss per 100 training steps: 0.31013021403091284\n",
      "Training loss per 100 training steps: 0.30981940338024144\n",
      "Training loss per 100 training steps: 0.3096714399322497\n",
      "Training loss per 100 training steps: 0.3095658365304615\n",
      "Training loss per 100 training steps: 0.30939180100062463\n",
      "Training loss per 100 training steps: 0.30897422855494056\n",
      "Training loss per 100 training steps: 0.30888251060644\n",
      "Training loss per 100 training steps: 0.30858818208060046\n",
      "Training loss per 100 training steps: 0.3083170334426368\n",
      "Training loss per 100 training steps: 0.3079460441867471\n",
      "Training loss per 100 training steps: 0.30760047976595994\n",
      "Training loss per 100 training steps: 0.30733153187095463\n",
      "Training loss per 100 training steps: 0.30698747634000884\n",
      "Training loss per 100 training steps: 0.30654173840494936\n",
      "Training loss per 100 training steps: 0.30642760747010234\n",
      "Training loss per 100 training steps: 0.30629469428970413\n",
      "Training loss per 100 training steps: 0.30622235910757506\n",
      "Training loss per 100 training steps: 0.3063720681207674\n",
      "Training loss per 100 training steps: 0.30597071951437727\n",
      "Training loss per 100 training steps: 0.3056951142440842\n",
      "Training loss per 100 training steps: 0.3054326634804571\n",
      "Training loss per 100 training steps: 0.30523680679068577\n",
      "Training loss per 100 training steps: 0.30525540650345095\n",
      "Training loss per 100 training steps: 0.30509669093830943\n",
      "Training loss per 100 training steps: 0.30496565549609844\n",
      "Training loss per 100 training steps: 0.3047223639821118\n",
      "Training loss per 100 training steps: 0.3046959014386865\n",
      "Training loss per 100 training steps: 0.3044288564358787\n",
      "Training loss per 100 training steps: 0.30420684180735086\n",
      "Training loss per 100 training steps: 0.3040481778701316\n",
      "Training loss per 100 training steps: 0.30382297849545864\n",
      "Training loss per 100 training steps: 0.30362099819617233\n",
      "Training loss per 100 training steps: 0.3034862269797647\n",
      "Training loss per 100 training steps: 0.30330243920296335\n",
      "Training loss per 100 training steps: 0.30316378847943204\n",
      "Training loss per 100 training steps: 0.30296255576606107\n",
      "Training loss per 100 training steps: 0.3029506220063226\n",
      "Training loss per 100 training steps: 0.3027895350088012\n",
      "Training loss per 100 training steps: 0.30261726559272717\n",
      "Training loss per 100 training steps: 0.30248295859798746\n",
      "Training loss per 100 training steps: 0.302345849994852\n",
      "Training loss per 100 training steps: 0.3021869503134255\n",
      "Training loss per 100 training steps: 0.30211004595983815\n",
      "Training loss per 100 training steps: 0.301715104648591\n",
      "Training loss per 100 training steps: 0.30156573816191024\n",
      "Training loss per 100 training steps: 0.30132440274484124\n",
      "Training loss per 100 training steps: 0.30109733107815817\n",
      "Training loss per 100 training steps: 0.30096219912083155\n",
      "Training loss per 100 training steps: 0.3005589039794087\n",
      "Training loss per 100 training steps: 0.3003378064432177\n",
      "Training loss per 100 training steps: 0.3003495141052804\n",
      "Training loss per 100 training steps: 0.3002196697769418\n",
      "Training loss per 100 training steps: 0.2999939472660118\n",
      "Training loss per 100 training steps: 0.2999812793419357\n",
      "Training loss per 100 training steps: 0.29977954985724\n",
      "Training loss per 100 training steps: 0.299536224693032\n",
      "Training loss per 100 training steps: 0.2993807710225001\n",
      "Training loss per 100 training steps: 0.29911198920280835\n",
      "Training loss per 100 training steps: 0.2990062413383899\n",
      "Training loss per 100 training steps: 0.2989955370759575\n",
      "Training loss per 100 training steps: 0.2987438779337699\n",
      "Training loss per 100 training steps: 0.2985547246443507\n",
      "Training loss per 100 training steps: 0.2983795778842747\n",
      "Training loss per 100 training steps: 0.29838126747697896\n",
      "Training loss per 100 training steps: 0.29826322905727376\n",
      "Training loss per 100 training steps: 0.29812849500866073\n",
      "Training loss per 100 training steps: 0.29785637461997577\n",
      "Training loss per 100 training steps: 0.2976703184050511\n",
      "Training loss per 100 training steps: 0.297525736645307\n",
      "Training loss per 100 training steps: 0.29741658287250866\n",
      "Training loss per 100 training steps: 0.2973114482649253\n",
      "Training loss per 100 training steps: 0.29705104584047404\n",
      "Training loss per 100 training steps: 0.2968893745724762\n",
      "Training loss per 100 training steps: 0.2967961606823824\n",
      "Training loss per 100 training steps: 0.29675887978271553\n",
      "Training loss per 100 training steps: 0.2966955675366194\n",
      "Training loss per 100 training steps: 0.2964217274791428\n",
      "Training loss per 100 training steps: 0.2962227236855989\n",
      "Training loss per 100 training steps: 0.29608307463518135\n",
      "Training loss per 100 training steps: 0.296017876270428\n",
      "Training loss per 100 training steps: 0.2959795756996647\n",
      "Training loss per 100 training steps: 0.2959202215050694\n",
      "Training loss per 100 training steps: 0.29587535647530033\n",
      "Training loss per 100 training steps: 0.29584439250119937\n",
      "Training loss per 100 training steps: 0.29571688267922064\n",
      "Training loss per 100 training steps: 0.29571790496963685\n",
      "Training loss per 100 training steps: 0.2956670598901939\n",
      "Training loss per 100 training steps: 0.2956720371155919\n",
      "Training loss per 100 training steps: 0.29565097475052404\n",
      "Training loss per 100 training steps: 0.29554559060820373\n",
      "Training loss per 100 training steps: 0.2955275583782888\n",
      "Training loss per 100 training steps: 0.29548195969930796\n",
      "Training loss per 100 training steps: 0.295398808812891\n",
      "Training loss per 100 training steps: 0.29523849933664104\n",
      "Training loss per 100 training steps: 0.29508174435204304\n",
      "Training loss per 100 training steps: 0.2950779761406339\n",
      "Training loss per 100 training steps: 0.2950571225764037\n",
      "Training loss per 100 training steps: 0.294890217012366\n",
      "Training loss per 100 training steps: 0.2947921159946895\n",
      "Training loss per 100 training steps: 0.2945923234239292\n",
      "Training loss per 100 training steps: 0.29453947233654204\n",
      "Training loss per 100 training steps: 0.29444539741306824\n",
      "Training loss per 100 training steps: 0.29424045665912085\n",
      "Training loss per 100 training steps: 0.29414668442008773\n",
      "Training loss per 100 training steps: 0.2939723448442009\n",
      "Training loss per 100 training steps: 0.29388306179803964\n",
      "Training loss per 100 training steps: 0.29380715819741177\n",
      "Training loss per 100 training steps: 0.2937134352832807\n",
      "Training loss per 100 training steps: 0.2935634271854687\n",
      "Training loss per 100 training steps: 0.2933852796571061\n",
      "Training loss per 100 training steps: 0.2932150212629102\n",
      "Training loss per 100 training steps: 0.29308955054678243\n",
      "Training loss per 100 training steps: 0.29298591407386737\n",
      "Training loss per 100 training steps: 0.2928839725378416\n",
      "Training loss per 100 training steps: 0.29284175463511775\n",
      "Training loss per 100 training steps: 0.29285752610333116\n",
      "Training loss per 100 training steps: 0.29279355441722166\n",
      "Training loss per 100 training steps: 0.29268537756933954\n",
      "Training loss per 100 training steps: 0.2925716951835711\n",
      "Training loss per 100 training steps: 0.2924804323972045\n",
      "Training loss per 100 training steps: 0.2924525686946769\n",
      "Training loss per 100 training steps: 0.29236066999750926\n",
      "Training loss per 100 training steps: 0.29233777100192854\n",
      "Training loss per 100 training steps: 0.2922376116347559\n",
      "Training loss per 100 training steps: 0.2921703149397524\n",
      "Training loss per 100 training steps: 0.29193730977670157\n",
      "Training loss per 100 training steps: 0.29184752475332243\n",
      "Training loss per 100 training steps: 0.29178796299030146\n",
      "Training loss per 100 training steps: 0.29169412161510466\n",
      "Training loss per 100 training steps: 0.2917259048058728\n",
      "Training loss per 100 training steps: 0.2917152094053057\n",
      "Training loss per 100 training steps: 0.2915680470581408\n",
      "Training loss per 100 training steps: 0.2914523291737884\n",
      "Training loss per 100 training steps: 0.2913598905971893\n",
      "Training loss per 100 training steps: 0.29137367026470457\n",
      "Training loss per 100 training steps: 0.29130192054568077\n",
      "Training loss per 100 training steps: 0.2913593569144178\n",
      "Training loss per 100 training steps: 0.2912508670996532\n",
      "Training loss per 100 training steps: 0.2912335077414284\n",
      "Training loss per 100 training steps: 0.29106918095071327\n",
      "Training loss per 100 training steps: 0.29109079461669474\n",
      "Training loss per 100 training steps: 0.29085627274570847\n",
      "Training loss per 100 training steps: 0.29084349732959724\n",
      "Training loss per 100 training steps: 0.2907455304896875\n",
      "Training loss per 100 training steps: 0.29066305569334083\n",
      "Training loss per 100 training steps: 0.2905007578996295\n",
      "Training loss per 100 training steps: 0.29047248595710795\n",
      "Training loss per 100 training steps: 0.29041961955572526\n",
      "Training loss per 100 training steps: 0.29041033736864014\n",
      "Training loss per 100 training steps: 0.2903140966792144\n",
      "Training loss per 100 training steps: 0.29031932575221414\n",
      "Training loss per 100 training steps: 0.29036396080175914\n",
      "Training loss per 100 training steps: 0.29022272467892757\n",
      "Training loss per 100 training steps: 0.29013181164897456\n",
      "Training loss per 100 training steps: 0.2899843571182933\n",
      "Training loss per 100 training steps: 0.2899802492445406\n",
      "Training loss per 100 training steps: 0.2898111048022092\n",
      "Training loss per 100 training steps: 0.289725026464549\n",
      "Training loss per 100 training steps: 0.28967006596329437\n",
      "Training loss per 100 training steps: 0.2896025898148028\n",
      "Training loss per 100 training steps: 0.2895629726176407\n",
      "Training loss per 100 training steps: 0.28953660763703765\n",
      "Training loss per 100 training steps: 0.2894400091975253\n",
      "Training loss per 100 training steps: 0.28936842004694585\n",
      "Training loss per 100 training steps: 0.28926295371201494\n",
      "Training loss per 100 training steps: 0.28911667088888954\n",
      "Training loss per 100 training steps: 0.28913400123712535\n",
      "Training loss per 100 training steps: 0.28912958764089036\n",
      "Training loss per 100 training steps: 0.2890220281124979\n",
      "Training loss per 100 training steps: 0.2888919593789824\n",
      "Training loss per 100 training steps: 0.2888005457396359\n",
      "Training loss per 100 training steps: 0.2886711925673249\n",
      "Training loss per 100 training steps: 0.28860686493680643\n",
      "Training loss per 100 training steps: 0.2885997618161501\n",
      "Training loss per 100 training steps: 0.28854369113693523\n",
      "Training loss per 100 training steps: 0.28837668080030976\n",
      "Training loss per 100 training steps: 0.2883230822927123\n",
      "Training loss per 100 training steps: 0.28841734676242914\n",
      "Training loss per 100 training steps: 0.2882746519732163\n",
      "Training loss per 100 training steps: 0.28827637951621804\n",
      "Training loss per 100 training steps: 0.2881308283143551\n",
      "Training loss per 100 training steps: 0.28814614591792426\n",
      "Training loss per 100 training steps: 0.28795356477489364\n",
      "Training loss per 100 training steps: 0.28782602706203\n",
      "Training loss per 100 training steps: 0.2878099857218837\n",
      "Training loss per 100 training steps: 0.28766991884490994\n",
      "Training loss per 100 training steps: 0.28771220613802456\n",
      "Training loss per 100 training steps: 0.2876885742347864\n",
      "Training loss per 100 training steps: 0.287633813998365\n",
      "Training loss per 100 training steps: 0.28757172782387425\n",
      "Training loss per 100 training steps: 0.2874882978845099\n",
      "Training loss per 100 training steps: 0.2874519919511044\n",
      "Training loss per 100 training steps: 0.287333908992235\n",
      "Training loss per 100 training steps: 0.28726636562236624\n",
      "Training loss per 100 training steps: 0.2871588265464525\n",
      "Training loss per 100 training steps: 0.2870654646474001\n",
      "Training loss per 100 training steps: 0.28708483503934623\n",
      "Training loss per 100 training steps: 0.2870235791954156\n",
      "Training loss per 100 training steps: 0.2869601009790631\n",
      "Training loss per 100 training steps: 0.2868744139877968\n",
      "Training loss per 100 training steps: 0.2867961003181254\n",
      "Training loss per 100 training steps: 0.286748933208004\n",
      "Training loss per 100 training steps: 0.28666635329780393\n",
      "Training loss per 100 training steps: 0.28659603808825873\n",
      "Training loss per 100 training steps: 0.2865794674432854\n",
      "Training loss per 100 training steps: 0.2865090889309995\n",
      "Training loss per 100 training steps: 0.28658395068008635\n",
      "Training loss per 100 training steps: 0.2864482585711693\n",
      "Training loss per 100 training steps: 0.28641823245899944\n",
      "Training loss per 100 training steps: 0.2863651390629289\n",
      "Training loss per 100 training steps: 0.286345498987177\n",
      "Training loss per 100 training steps: 0.28627640301335555\n",
      "Training loss per 100 training steps: 0.28616984678779667\n",
      "Training loss per 100 training steps: 0.28608506487547636\n",
      "Training loss per 100 training steps: 0.28597160675230077\n",
      "Training loss per 100 training steps: 0.2859994818951131\n",
      "Training loss per 100 training steps: 0.2859131320322131\n",
      "Training loss per 100 training steps: 0.28586769591006334\n",
      "Training loss per 100 training steps: 0.2858281762081724\n",
      "Training loss per 100 training steps: 0.28575415559797523\n",
      "Training loss per 100 training steps: 0.28563807771532157\n",
      "Training loss per 100 training steps: 0.28561417220014756\n",
      "Training loss per 100 training steps: 0.2855970779261338\n",
      "Training loss per 100 training steps: 0.28546083446788306\n",
      "Training loss per 100 training steps: 0.2854325115016575\n",
      "Training loss per 100 training steps: 0.28541663469169887\n",
      "Training loss per 100 training steps: 0.28539885987167635\n",
      "Training loss per 100 training steps: 0.2854001838109262\n",
      "Training loss per 100 training steps: 0.285386399571405\n",
      "Training loss per 100 training steps: 0.2853264613680243\n",
      "Training loss per 100 training steps: 0.2852924114152508\n",
      "Training loss per 100 training steps: 0.28535431367110203\n",
      "Training loss per 100 training steps: 0.2852995759168189\n",
      "Training loss per 100 training steps: 0.2852100096049906\n",
      "Training loss per 100 training steps: 0.2850579463928329\n",
      "Training loss per 100 training steps: 0.28505025594368305\n",
      "Training loss per 100 training steps: 0.2850772183359414\n",
      "Training loss per 100 training steps: 0.2850151766095182\n",
      "Training loss per 100 training steps: 0.2849505978204934\n",
      "Training loss per 100 training steps: 0.2848858241770155\n",
      "Training loss per 100 training steps: 0.28479355287839203\n",
      "Training loss per 100 training steps: 0.2846872772712341\n",
      "Training loss per 100 training steps: 0.2846691542845588\n",
      "Training loss per 100 training steps: 0.2845536952852189\n",
      "Training loss per 100 training steps: 0.2844668266459903\n",
      "Training loss per 100 training steps: 0.28448296609289964\n",
      "Training loss per 100 training steps: 0.28437347352342746\n",
      "Training loss per 100 training steps: 0.2843833284256742\n",
      "Training loss per 100 training steps: 0.28432408777121965\n",
      "Training loss per 100 training steps: 0.28428678048172434\n",
      "Training loss per 100 training steps: 0.28425465007004086\n",
      "Training loss per 100 training steps: 0.28424201568449775\n",
      "Training loss per 100 training steps: 0.284189968519589\n",
      "Training loss per 100 training steps: 0.28414209090741444\n",
      "Training loss per 100 training steps: 0.28413888453769537\n",
      "Training loss per 100 training steps: 0.2840583004537581\n",
      "Training loss per 100 training steps: 0.2839916713413715\n",
      "Training loss per 100 training steps: 0.2839722691856824\n",
      "Training loss per 100 training steps: 0.2839513293536772\n",
      "Training loss per 100 training steps: 0.28394468124978234\n",
      "Training loss per 100 training steps: 0.28389470557490504\n",
      "Training loss per 100 training steps: 0.28380608601292817\n",
      "Training loss per 100 training steps: 0.28368645384534424\n",
      "Training loss per 100 training steps: 0.2835728637153018\n",
      "Training loss per 100 training steps: 0.28347297346577804\n",
      "Training loss per 100 training steps: 0.2833825342551985\n",
      "Training loss per 100 training steps: 0.2833226687943816\n",
      "Training loss per 100 training steps: 0.2832910498617457\n",
      "Training loss per 100 training steps: 0.28322473978220103\n",
      "Training loss per 100 training steps: 0.2832587245118142\n",
      "Training loss per 100 training steps: 0.28321938238896543\n",
      "Training loss per 100 training steps: 0.28321053107269845\n",
      "Training loss per 100 training steps: 0.2831793553567655\n",
      "Training loss per 100 training steps: 0.28312933407061674\n",
      "Training loss per 100 training steps: 0.28306415348853525\n",
      "Training loss per 100 training steps: 0.2830429341136366\n",
      "Training loss per 100 training steps: 0.28300968996271086\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c4587b-a7f4-49df-a84f-5e6818133adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('model/sri_biobert-base-cased-v1.1')\n",
    "tokenizer.save_pretrained('token/sri_biobert-base-cased-v1.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177db691-31c5-4691-8f53-b3702bb607f7",
   "metadata": {},
   "source": [
    "# PMC-Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8c33a-f67f-442b-84d7-2692880ea377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b21b86d-dbed-440c-b39e-4555cdf4fd8a",
   "metadata": {},
   "source": [
    "Patient information are presented as a dataframe \n",
    "- `patient_id`: string. A continuous id of patients, starting from 0.\n",
    "- `patient_uid`: string. Unique ID for each patient, with format PMID-x, where PMID is the PubMed Identifier of source article of the note and x denotes index of the note in source article.\n",
    "- `PMID`: string. PMID for source article.\n",
    "- `file_path`: string. File path of xml file of source article.\n",
    "- `title`: string. Source article title.\n",
    "- `patient`: string. Patient note.\n",
    "- `age`: list of tuples. Each entry is in format `(value, unit)` where value is a float number and unit is in 'year', 'month', 'week', 'day' and 'hour' indicating age unit. For example, `[[1.0, 'year'], [2.0, 'month']]` indicating the patient is a one-year- and two-month-old infant.\n",
    "- `gender`: 'M' or 'F'. Male or Female.\n",
    "- `relevant_articles`: dict. The key is PMID of the relevant articles and the corresponding value is its relevance score (2 or 1 as defined in the ``Methods'' section).\n",
    "- `similar_patients`: dict. The key is patient_uid of the similar patients and the corresponding value is its similarity score (2 or 1 as defined in the ``Methods'' section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f64934-9da7-4bb4-a89b-cd1e123d98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import fhirclient.models.patient as fhir_patient\n",
    "import fhirclient.models.condition as fhir_condition\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "import re\n",
    "import nltk\n",
    "from autocorrect import Speller\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import Speller\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa72a0-0b7f-4c78-b5f4-abc41f058c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8066c4f-047e-491a-a825-42c65c8e47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d23ab-8073-47b6-850f-f4c100ac73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "biomodel = HuggingFaceEndpoint(\n",
    "   # repo_id=\"dmis-lab/biobert-v1.1\",\n",
    "    repo_id=\"alvaroalon2/biobert_diseases_ner\",\n",
    "    temperature=0,\n",
    "    model_kwargs={\"max_length\": 180, \"device\": \"cuda\"},\n",
    "    use_auth_token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72d5ea-eec6-412f-9d49-e0193981a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = Ollama(\n",
    "    base_url='http://10.113.8.4:8086',\n",
    "    model=\"cniongolo/biomistral:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092d91c-d7f4-40f5-921c-ec2b9a19c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"zhengyun21/PMC-Patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85352519-c071-4e27-980f-2ef8be87fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f273a-7291-4b5e-96c9-e82b2e13464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aceeb8e-71f8-4500-bb35-a75e98297f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94bee3c-446f-4922-b683-e9dff7e2198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720a069-8c8e-41f9-bf1c-8c165c6c47e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e20c7-1310-43a1-8ef0-35150a0d89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b11bc3-a512-4f15-ba2e-c71f970c7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d19767-c6f7-43c4-88eb-03538848753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7bc92c-4c51-4bd7-a50c-521c0489cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# age\n",
    "extract_age = lambda x: re.findall(r\"\\d+\\.\\d+\", x)[0]\n",
    "df.age = df.age.apply(extract_age).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd088c8a-d72c-447c-bab8-1354ce40584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ff355-66a8-4947-a248-8f49f368bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns=[\"file_path\", \"patient_id\", \"patient_uid\", \"relevant_articles\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0464cd2-322f-42d6-a469-0f3f00271a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_note = \"\"\"\n",
    "The patient is a 45-year-old male with a history of diabetes and hypertension.\n",
    "He was prescribed metformin and lisinopril.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e840b205-b21b-41ef-aa6e-d399a0ee0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\"\n",
    "stop_words  = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e2b51-aab9-49ff-b267-e89a468dd028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Converting text to lower case as in, converting \"Hello\" to  \"hello\" or \"HELLO\" to \"hello\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Specll check the words\n",
    "    spell  = Speller(lang='en')\n",
    "    \n",
    "    texts = spell(text)\n",
    "    \n",
    "    return ' '.join([w.lower() for w in word_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a7298a-1b74-4858-ba98-2b00b2d14697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(lower_case):\n",
    "    # split text phrases into words\n",
    "    words  = nltk.word_tokenize(lower_case)\n",
    "    \n",
    "    \n",
    "    # Create a list of all the punctuations\n",
    "    punctuations = [ '/', '!', '?', ';', ':', '(',')', '[',']', '-', '_', '%']\n",
    "    \n",
    "    # Remove all the special characters\n",
    "    punctuations = re.sub(r'\\W', ' ', str(lower_case))\n",
    "    \n",
    "    # Initialize the stopwords variable, which is a list of words ('and', 'the', 'i', 'yourself', 'is') that do not hold much values as key words\n",
    "    stop_words  = stopwords.words('english')\n",
    "    \n",
    "    # Getting rid of all the words that contain numbers in them\n",
    "    w_num = re.sub('\\w*\\d\\w*', '', lower_case).strip()\n",
    "    \n",
    "    # remove all single characters\n",
    "    lower_case = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', lower_case)\n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    lower_case = re.sub(r'\\s+', ' ', lower_case, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    lower_case = re.sub(r'^b\\s+', '', lower_case)\n",
    "    \n",
    "    # Removing non-english characters\n",
    "    lower_case = re.sub(r'^b\\s+', '', lower_case)\n",
    "    \n",
    "    # Return keywords which are not in stop words \n",
    "    keywords = [word for word in words if not word in stop_words  and word in punctuations and  word in w_num]\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d1a9e-2254-4fdd-ae66-88a1fa4905c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14f3b7-2f00-452d-8344-8de6b3f84940",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "print('We will use the GPU:', torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c8a3a6-2f23-484a-b98c-ee7772291282",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/sridhanya_ganapathi_team_neustar/PMC-Patients/ddata/datasets/NER/BC5CDR-disease/train.tsv\", sep=\"\\t\").fillna(method=\"ffill\")\n",
    "# data['Value'][0] = 'B-Chemical'\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc884e-a14a-4829-9109-a45516cf191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# !pip install transformers\n",
    "# !pip install torch\n",
    " \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    " \n",
    "# Load the BioBERT model and tokenizer\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    " \n",
    "# Define the NER pipeline using the BioBERT model\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    " \n",
    "# Example biomedical text\n",
    "text = \"\"\"\n",
    "A 45-year-old male patient was diagnosed with hypertension and prescribed Lisinopril.\n",
    "He also has a history of diabetes and his blood test showed elevated glucose levels.\n",
    "\"\"\"\n",
    "text=clean_text(to_lower(clinical_note))\n",
    "text=\" \".join(text)\n",
    " \n",
    "# Perform Named Entity Recognition (NER) on the text\n",
    "entities = ner_pipeline(text)\n",
    " \n",
    "# Print the detected entities\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Type: {entity['entity_group']}, Confidence: {entity['score']:.4f}\")\n",
    "#Label_1 typically represents medical text.\n",
    "#Label_0 typically represents non-medical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da38d8-fa5a-4695-8bad-14eaa91572bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\" -O biobert_weights && rm -rf /tmp/cookies.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2943e83-48b2-470b-af8d-21b7e2788bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816b441-724e-4145-967b-20fdde4ba3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained('axiong/PMC_LLaMA_13B')\n",
    "model = transformers.LlamaForCausalLM.from_pretrained('axiong/PMC_LLaMA_13B')\n",
    "model.cuda()  # move the model to GPU\n",
    "\n",
    "prompt_input = (\n",
    "    'Below is an instruction that describes a task, paired with an input that provides further context.'\n",
    "    'Write a response that appropriately completes the request.\\n\\n'\n",
    "    '### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:'\n",
    ")\n",
    "\n",
    "example = {\n",
    "    \"instruction\": \"You're a doctor, kindly address the medical queries according to the patient's account. Answer with the best option directly.\",\n",
    "    \"input\": (\n",
    "        \"###Question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. \"\n",
    "        \"She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. \"\n",
    "        \"She otherwise feels well and is followed by a doctor for her pregnancy. \"\n",
    "        \"Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air.\"\n",
    "        \"Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. \"\n",
    "        \"Which of the following is the best treatment for this patient?\"\n",
    "        \"###Options: A. Ampicillin B. Ceftriaxone C. Doxycycline D. Nitrofurantoin\"\n",
    "    )\n",
    "}\n",
    "input_str = [prompt_input.format_map(example)]\n",
    "\n",
    "model_inputs = tokenizer(\n",
    "    input_str,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    ")\n",
    "print( f\"\\033[32mmodel_inputs\\033[0m: { model_inputs }\" )\n",
    "\n",
    "\n",
    "topk_output = model.generate(\n",
    "    model_inputs.input_ids.cuda(),\n",
    "    max_new_tokens=1000,\n",
    "    top_k=50\n",
    ")\n",
    "output_str = tokenizer.batch_decode(topk_output)\n",
    "print('model predict: ', output_str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5346fd8-49f9-49a3-88c5-bb0517509030",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://huggingface.co/kormilitzin/en_core_med7_lg/resolve/main/en_core_med7_lg-any-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d615c41-e1c3-4f6b-80ec-7aad96547ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import medspacy\n",
    "from medspacy.ner import TargetRule\n",
    "from medspacy.visualization import visualize_ent\n",
    "\n",
    "# Load medspacy model\n",
    "nlp = medspacy.load()\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "text = \"\"\"\n",
    "Past Medical History:\n",
    "1. Atrial fibrillation\n",
    "2. Type II Diabetes Mellitus\n",
    "\n",
    "Assessment and Plan:\n",
    "There is no evidence of pneumonia. Continue warfarin for Afib. Follow up for management of type 2 DM.\n",
    "\"\"\"\n",
    "\n",
    "# Add rules for target concept extraction\n",
    "target_matcher = nlp.get_pipe(\"medspacy_target_matcher\")\n",
    "target_rules = [\n",
    "    TargetRule(\"atrial fibrillation\", \"PROBLEM\"),\n",
    "    TargetRule(\"atrial fibrillation\", \"PROBLEM\", pattern=[{\"LOWER\": \"afib\"}]),\n",
    "    TargetRule(\"pneumonia\", \"PROBLEM\"),\n",
    "    TargetRule(\"Type II Diabetes Mellitus\", \"PROBLEM\", \n",
    "              pattern=[\n",
    "                  {\"LOWER\": \"type\"},\n",
    "                  {\"LOWER\": {\"IN\": [\"2\", \"ii\", \"two\"]}},\n",
    "                  {\"LOWER\": {\"IN\": [\"dm\", \"diabetes\"]}},\n",
    "                  {\"LOWER\": \"mellitus\", \"OP\": \"?\"}\n",
    "              ]),\n",
    "    TargetRule(\"warfarin\", \"MEDICATION\")\n",
    "]\n",
    "target_matcher.add(target_rules)\n",
    "\n",
    "doc = nlp(text)\n",
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c021cb77-3ba0-4a4a-a3b7-eb916a9c974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://huggingface.co/kormilitzin/en_core_med7_trf/blob/main/en_core_med7_trf-any-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43476849-2e0b-4fa8-b793-3e4030e53e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import medspacy\n",
    "from medspacy.ner import TargetRule\n",
    "from medspacy.visualization import visualize_ent\n",
    "\n",
    "# Load medspacy model\n",
    "nlp = medspacy.load()\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "text = \"\"\"\n",
    "Past Medical History:\n",
    "1. Atrial fibrillation\n",
    "2. Type II Diabetes Mellitus\n",
    "\n",
    "Assessment and Plan:\n",
    "There is no evidence of pneumonia. Continue warfarin for Afib. Follow up for management of type 2 DM.\n",
    "\"\"\"\n",
    "\n",
    "# Add rules for target concept extraction\n",
    "target_matcher = nlp.get_pipe(\"medspacy_target_matcher\")\n",
    "target_rules = [\n",
    "    TargetRule(\"atrial fibrillation\", \"PROBLEM\"),\n",
    "    TargetRule(\"atrial fibrillation\", \"PROBLEM\", pattern=[{\"LOWER\": \"afib\"}]),\n",
    "    TargetRule(\"pneumonia\", \"PROBLEM\"),\n",
    "    TargetRule(\"Type II Diabetes Mellitus\", \"PROBLEM\", \n",
    "              pattern=[\n",
    "                  {\"LOWER\": \"type\"},\n",
    "                  {\"LOWER\": {\"IN\": [\"2\", \"ii\", \"two\"]}},\n",
    "                  {\"LOWER\": {\"IN\": [\"dm\", \"diabetes\"]}},\n",
    "                  {\"LOWER\": \"mellitus\", \"OP\": \"?\"}\n",
    "              ]),\n",
    "    TargetRule(\"warfarin\", \"MEDICATION\")\n",
    "]\n",
    "target_matcher.add(target_rules)\n",
    "\n",
    "doc = nlp(text)\n",
    "visualize_ent(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e9056-370c-4568-b44a-fe90a49c6cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a67ed-c0b4-4ca3-bc8e-389f2dec2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load BioBERT NER model\n",
    "ner_model = pipeline('ner', model='d4data/biomedical-ner-all')\n",
    " \n",
    "# Input text\n",
    "text = \"The patient was diagnosed with glioblastoma and treated with temozolomide.\"\n",
    " \n",
    "# Extract entities\n",
    "entities = ner_model(text)\n",
    " \n",
    "# Output named entities\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b052eb0-bf32-459e-9288-9edb7b401cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8290e-c56e-48cc-b68e-a8b30ec6dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c11e8-2c94-42ae-a915-a8cb04799553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0) # pass device=0 if using gpu\n",
    "pipe(\"\"\"The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1a19e-292e-44b8-8a57-1db3e3e00ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scispacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86adf9d7-d3bc-4f22-b2c7-7a01441ea585",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_ner_bc5cdr_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12486b8c-ac6d-4ee2-8cc1-65ffdb84e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513e2d2-658e-45f3-9fe9-64ba16c75bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b063b09-37de-4df0-bfd0-7497e3aa7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740f146-3427-4dae-a204-bcc30bb95a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy==2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3172c16-8fb5-471e-ae4b-0c3f84dc3ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import srsly\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e880c-ca9f-415e-ace4-e0602153b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28446952-ed5a-4365-a428-0439eaea0b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://huggingface.co/kormilitzin/en_core_med7_lg/resolve/main/en_core_med7_lg-any-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1bdf0-8b76-43e7-ad10-b10c0978cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "from collections import OrderedDict\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "\n",
    "# Define the text\n",
    "text = \"\"\"\n",
    "The patient is well known to me for a history of iron-deficiency anemia due to chronic blood loss from colitis. We corrected her hematocrit last year with intravenous (IV) iron. Ultimately, she had a total proctocolectomy done on 03/14/2007 to treat her colitis. Her course has been very complicated since then with needing multiple surgeries for removal of hematoma. This is partly because she was on anticoagulation for a right arm deep venous thrombosis (DVT) she had early this year, complicated by septic phlebitis. Chart was reviewed, and I will not reiterate her complex history. I am asked to see the patient again because of concerns for coagulopathy. She had surgery again last month to evacuate a pelvic hematoma, and was found to have vancomycin resistant enterococcus, for which she is on multiple antibiotics and followed by infectious disease now. She is on total parenteral nutrition (TPN) as well. LABORATORY DATA: Labs today showed a white blood cell count of 12,000.\n",
    "\"\"\"\n",
    "\n",
    "# Set up stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize and filter stopwords\n",
    "word_tokens = word_tokenize(text)\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "# Ensure the filtered_sentence list is populated correctly\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w.lower() not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# Load the UMLS Entity Linker\n",
    "linker = UmlsEntityLinker(k=10, max_entities_per_mention=2)\n",
    "\n",
    "# Register the extension attribute\n",
    "if not Doc.has_extension(\"umls_ents\"):\n",
    "    Doc.set_extension(\"umls_ents\", default=[])\n",
    "\n",
    "# Add the linker to the pipeline\n",
    "nlp.add_pipe(\"scispacy_linker\", last=True)\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract entities\n",
    "entities = doc.ents\n",
    "entity_texts = [str(item) for item in entities]\n",
    "\n",
    "# Create an ordered dictionary of entities\n",
    "entity_dict = OrderedDict.fromkeys(entity_texts)\n",
    "\n",
    "# Process the entities with spaCy\n",
    "entity_doc = nlp(\" \".join(entity_dict.keys()))\n",
    "\n",
    "# Print the entities and their UMLS concepts\n",
    "for entity in entity_doc.ents:\n",
    "    if entity._.umls_ents:  # Check if umls_ents attribute is present\n",
    "        for umls_ent in entity._.umls_ents:\n",
    "            print(\"Entity_name:\", entity.text)\n",
    "            concept_id, score = umls_ent\n",
    "            print(\"concept_id={} Score={}\".format(concept_id, score))\n",
    "    else:\n",
    "        print(f\"No UMLS entities found for: {entity.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d733873-e71e-4c73-9f65-e5cf5c7c148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Register the extension attribute\n",
    "if Doc.has_extension(\"umls_ents\"):\n",
    "    print(\"not\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c0219-911f-4f5f-9c89-bd6d01bd130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739d89d-736c-49f9-91d5-a5477bf023e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b99fb38-0100-4173-9e38-0448ece9eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6097652-c458-48ee-b65f-a061ccbfa3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.umls_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b4fd5-2a2a-4e25-ba6c-f299939ea52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_doc._.umls_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d78bd9-2fa8-438b-ad61-b5803d0515b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz\n",
    "!pip install https://huggingface.co/kormilitzin/en_core_med7_trf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a75b96-43e4-4ca6-bd46-fbb17768bbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640761d5-e9f4-4731-9386-a6ce96aa8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://huggingface.co/kormilitzin/en_core_med7_lg/resolve/main/en_core_med7_lg-any-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048dd17-42ff-4ea5-8d9d-811d74202a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_md-0.5.1.tar.gz\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bc5cdr_md-0.5.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0e0d2a-a019-4ef8-85b3-e018908e30a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
